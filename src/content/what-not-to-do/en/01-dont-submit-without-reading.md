---
title: "Don't Submit AI Output Without Reading Every Word"
number: 1
description: "AI generates text that sounds authoritative and professional. This makes it dangerously easy to assume the output is correct without actually reading it carefully. In insurance, unverified AI output can result in incorrect coverage determinations, regulatory violations, and financial exposure."
risk: "Incorrect coverage decisions, regulatory non-compliance, financial losses, professional liability, damaged client relationships"
realExample: "In the landmark Mata v. Avianca case (2023), a New York attorney submitted a legal brief containing AI-fabricated case citations — cases that simply did not exist. The attorney was sanctioned by the court. While this occurred in the legal field, the exact same risk applies to insurance: an AI could generate a non-existent regulation, misquote a policy provision, fabricate actuarial data, or invent an industry standard. If you submit it without reading it, the liability is yours."
mitigation: "Read every word of every AI output you intend to use professionally. Verify all factual claims, regulation references, policy citations, and data points against primary sources. If you don't have time to verify an AI output, you don't have time to use it."
insureversiasTake: "This is rule number one for a reason. AI is the most convincing unverified source you will ever encounter. It writes beautifully, sounds authoritative, and is wrong just often enough to be dangerous. The Mata v. Avianca attorney didn't submit fabricated citations because he was careless — he submitted them because they looked completely real. Your AI-generated coverage analysis will look just as real. Read it. Verify it. Every time."
sources:
  - type: news
    title: "Lawyers Sanctioned for Using AI-Generated Fake Citations in Court Filing"
    author: "Benjamin Weiser"
    date: "2023-06-22"
    url: "https://www.nytimes.com/2023/06/22/nyregion/chatgpt-ai-lawyers-sanctions.html"
    note: "The Mata v. Avianca case that demonstrated the risks of unverified AI output in professional practice"
  - type: regulation
    title: "NAIC Model Bulletin on the Use of Artificial Intelligence"
    author: "National Association of Insurance Commissioners"
    date: "2023-12-04"
    note: "Establishes expectations for AI governance including human oversight requirements"
---

## The Danger in Detail

The fundamental problem is this: AI generates text by predicting what words should come next, based on statistical patterns. It does not verify facts. It does not check whether the regulation it cited actually exists. It does not confirm that the policy provision it quoted matches the actual policy language. It produces text that is statistically probable, not factually verified.

### What Can Go Wrong in Insurance

**Fabricated regulations**: AI might cite a "NAIC Model Law on Algorithmic Accountability" that does not exist, or misstate the requirements of a real regulation.

**Misquoted policy language**: AI might paraphrase an exclusion in a way that changes its legal meaning, or invent provisions that aren't in the policy.

**Incorrect calculations**: Reserve recommendations, premium calculations, or loss ratio analyses that look correct but contain mathematical errors or wrong assumptions.

**Invented precedents**: References to regulatory actions, market conduct examinations, or industry standards that sound real but are fabricated.

### The Verification Protocol

For every AI output you intend to use:

1. **Read the entire output** — not just the conclusions, but every supporting detail.
2. **Flag every factual claim** — regulations cited, data points, policy references, industry standards.
3. **Verify each flagged item** against the primary source.
4. **Check the reasoning** — does the logic hold up, or did the AI make an unsupported logical leap?
5. **Document your verification** — especially for outputs used in coverage determinations or regulatory submissions.
