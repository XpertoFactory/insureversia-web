---
title: "Don't Use AI for Tasks Requiring Emotional Intelligence"
number: 7
description: "Insurance touches people at their most vulnerable moments — after a car accident, a house fire, a health crisis, the death of a loved one. AI cannot provide empathy, read emotional cues, or respond with the human sensitivity these moments demand."
risk: "Damaged policyholder relationships, complaints, E&O exposure, regulatory scrutiny for unfair claims practices, reputational harm"
realExample: "In 2023, the National Eating Disorders Association (NEDA) replaced its human helpline counselors with an AI chatbot named 'Tessa.' Within days, users reported that Tessa was providing advice that could worsen eating disorders — including recommending calorie counting and weight loss to people seeking help for anorexia. NEDA shut Tessa down within a week. In insurance, the parallel is clear: using AI chatbots or automated communications for claims involving serious injury, death, or catastrophic loss carries the same risk of providing technically-generated but emotionally inappropriate responses."
mitigation: "Reserve tasks that require empathy, emotional intelligence, and human connection for human professionals. Use AI for background analysis and drafting that supports these interactions, but never as a substitute for the human interaction itself."
insureversiasTake: "Your policyholder just lost their home in a fire. Everything they owned. Their children's photos. Their grandmother's jewelry. They call to file a claim. Do you want them talking to a chatbot? The AI might process the claim faster. It might ask all the right questions. But it cannot understand what that policyholder is going through. It cannot pause, soften its tone, and say 'I'm so sorry — let's take this one step at a time.' Use AI to do the paperwork so your people can do the people work."
sources:
  - type: news
    title: "An Eating Disorders Chatbot Offered Dieting Tips. Users Weren't Happy."
    author: "Sapna Maheshwari"
    date: "2023-06-08"
    url: "https://www.nytimes.com/2023/06/08/business/eating-disorder-chatbot-neda.html"
    note: "NEDA's AI chatbot provided harmful advice, illustrating the danger of AI in emotionally sensitive contexts"
  - type: regulation
    title: "NAIC Unfair Claims Settlement Practices Model Act"
    note: "Requires fair and prompt claims handling, including reasonable communication with claimants"
---

## The Danger in Detail

Insurance is a human profession that deals with human crises. Property losses, liability claims, health emergencies, disability, death — these are not abstract data processing tasks. They are moments of profound personal impact for the people involved.

### What Emotional Intelligence Provides

**Active listening**: Understanding not just what a claimant says, but what they mean, what they fear, and what they need to hear. A claimant who says "I need my claim paid now" may actually mean "I'm terrified I can't afford to rebuild."

**Empathetic communication**: Delivering difficult news — a coverage denial, a lower-than-expected settlement, a long investigation timeline — requires sensitivity to the person's emotional state.

**Rapport building**: Trust between insurer and policyholder is built through human interaction — demonstrations of genuine concern and the sense that someone cares about their situation.

**Cultural sensitivity**: Policyholders from different backgrounds may communicate differently about their losses. Cultural attitudes toward institutions, authority, and conflict require human attunement.

### Where AI Can Help (Without Replacing You)

AI can support emotionally demanding work without being the face of it:

- Prepare a summary of the claim file before your conversation with the claimant.
- Draft the follow-up letter *after* you've had the empathetic conversation.
- Research comparable claims to support the settlement you'll discuss in person.
- Generate a plain-language explanation of coverage that you then deliver personally.
- Handle the administrative paperwork so you can spend more time on the human dimensions.

The technology's highest use is freeing you to be more human, not less.
