---
title: "Establish an Organization-Wide AI Use Policy"
number: 3
description: "Ad hoc AI use across an organization creates inconsistency, compliance risk, and potential data breaches. A clear, written AI use policy establishes guardrails that protect your organization while enabling productive AI adoption."
howTo: "Draft and implement a comprehensive AI use policy that covers approved tools, data handling requirements, verification procedures, disclosure obligations, and accountability frameworks. Review and update the policy quarterly."
steps:
  - "Audit current AI use across your organization â€” who is using what, and for which tasks."
  - "Identify data sensitivity levels: what can and cannot be entered into AI tools."
  - "Define approved AI tools and platforms based on security, privacy, and compliance requirements."
  - "Establish verification protocols for different output types (internal analysis, customer-facing, regulatory submissions)."
  - "Create clear accountability: who is responsible for AI-assisted decisions?"
  - "Set training requirements for all staff who use AI tools."
  - "Schedule quarterly policy reviews to address new tools, regulations, and lessons learned."
tools:
  - "Internal policy templates"
  - "NAIC Model Bulletin (framework reference)"
  - "ISO 42001 (AI management system standard)"
---

## Why This Matters

Without a policy, AI adoption happens chaotically. Some employees use consumer-grade tools with policyholder data. Others avoid AI entirely out of caution. Different teams produce inconsistent outputs. And when something goes wrong, there's no framework for accountability.

### Key Policy Components

**Approved Tools**: List the AI tools approved for use, categorized by data sensitivity level. Consumer-grade tools (free ChatGPT) may be acceptable for general research but not for tasks involving policyholder data.

**Data Classification**: Define what data can be entered into which AI tools. Policyholder PII, claims data, and proprietary pricing models typically require enterprise-grade tools with appropriate data processing agreements.

**Verification Requirements**: Different outputs require different levels of verification. An internal brainstorming output needs less scrutiny than a coverage determination that affects a policyholder.

**Disclosure Obligations**: When must AI use be disclosed? To policyholders? To regulators? To reinsurers? Define clear rules based on your regulatory environment.

**Training and Competency**: Who is required to complete AI training before using tools? What does the training cover? How is competency assessed?

**Incident Response**: What happens when an AI-related error occurs? Define reporting, investigation, and remediation procedures.
